{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFzU2cdBQula"
      },
      "source": [
        "# **NLG Using LSTM**\n",
        "\n",
        "---\n",
        "\n",
        "Natural Language Generation (NLG) is a software process that produces natural language output. NLG is the subfield of artificial intelligence and computational linguistics that is concerned with the construction of computer systems that can produce understandable texts in English or other human languages from some underlying non-linguistic representation of information.\n",
        "\n",
        "Long short-term memory (LSTM) is a type of recurrent neural network (RNN) aimed at dealing with the vanishing gradient problem present in traditional RNNs.  LSTM can be used to predict the next word in a sequence of words. The neural network takes a sequence of words as input and outputs a matrix of probability for each word from the dictionary to be the next word of the given sequence. The model learns how much similarity there is between each word or character and calculates the probability of each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Assignment Instructions**\n",
        "\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "### **Introduction**\n",
        "\n",
        "For this assignment, you will train a long short-term memory (LSTM) module that serves as a language model processing engine.  To train the model, report on its progress, and use it to generate text, the training process will generate a pair of files for each epoch completed and labeled for the corresponding epoch:\n",
        "\n",
        "  *  **Losses**.  This file records the average loss calculated during the training for each epoch.  This file is used when plotting the loss curve.  This file takes the format of `losses_{epoch}.txt`.\n",
        "\n",
        "  *  **Model**.  This is the LSTM model resulting from the training for each epoch.  This file takes the format of `lstm_model_{epoch}.pth`.\n",
        "\n",
        "The time required to train the LSTM model is non-trivial.  It is possible&mdash;and almost certain&mdash;that your Colab runtime will time-out and disconnect before the model training is complete.  This will also delete any files in the Colab filespace.\n",
        "\n",
        "To remedy this problem, you will need to create a dedicated folder in your Google Drive that this Colab notebook can access and where it can read and write files.  This will ensure that the necessary files, especially the model file, will not be automatically deleted if, and when, the Colab runtime disconnects.  The set-up instructions below explain how to create the Google Drive folder.  If you follow these instructions, this Colab will automatically recognize and connect to the location.\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Set-up Google Drive**\n",
        "\n",
        "Complete these steps to create the necessary Google Drive folder for this assignment:\n",
        "\n",
        "1.   Open your Google Drive.  This will put you in your `MyDrive` folder by default.\n",
        "2.   Click the `+ New` button in the upper left corner.\n",
        "3.   Select `New folder`.\n",
        "4.   In the dialog box that appears, type the name of the folder:  `LING 581`\n",
        "5.   Click `Create`.\n",
        "6.   Open the new `LING 581` folder you just created.\n",
        "7.   Click the `+ New` button in the upper left corner.\n",
        "8.   Select `New folder`.\n",
        "9.   In the dialog box that appears, type the name of the folder:  `LSTM`\n",
        "10.   Click `Create`.\n",
        "\n",
        "This will ensure that your Google Drive has the necessary file path specified in Code Step 3 below:\n",
        "\n",
        "    directory_path = '/content/drive/MyDrive/LING 581/LSTM/'\n",
        "\n",
        "With your Google Drive properly configured, your LSTM files will be preserved and the LSTM training can resume from the last completed epoch should it get interrupted before it completes.\n",
        "\n",
        "**Note that when you run this Colab, you will be prompted to give access to your Google Drive so that the training can read and write files that will persist beyond the life of the runtime.**\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Create the Assignment Report**\n",
        "\n",
        "For this assignment, you will need the following resource:\n",
        "\n",
        "*  `1 Nephi Chaps 1_4.txt`\n",
        "\n",
        "To receive full credit for this assignment, you will need to create and submit the following:\n",
        "\n",
        "1.   A report document that contains the full information and correct answers to the questions contained in Parts I, II, and III below.  Submit a PDF version of that document.  \n",
        "\n",
        "  *  At the beginning of your report, include a short description of the text file.\n",
        "\n",
        "  *  At the end of your report, provide a brief but detailed summary that includes:\n",
        "      *   A concise overall analysis of the data from each step of your language processing.\n",
        "      *   Your explanation of the value of using LSTM for natural language generation (NLG).\n",
        "      *   What you learned by doing this assignment.  \n",
        "      *   One suggestion for improving this assignment.\n",
        "2.   The URL of your copy of the Colab notebook (with View permissions) with your data results.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Part I:  Train the Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Complete these tasks:**\n",
        "\n",
        "1. Copy the text file to the `MyDrive/LING 581/LSTM` folder you created in your Google Drive.  Ensure that there are no other files in this folder.\n",
        "\n",
        "2.  Run code steps 1 - 4 below to start the training of the LSTM.  Do not modify any of the hyperparameters in code step 3.  Once you execute code step 4, the LSTM model training will begin.  Training the LSTM has been improved and will only take a minute or two.  \n",
        "  \n",
        "  As you have seen before, there are three runtime options for a free Colab account.  **The CPU option takes the longest** during peak hours with the provided text file.  During non-peak hours, the time required to train the model can be significantly less with each runtime option, but plan accordingly.\n",
        "\n",
        "  If the Colab runtime disconnects before completing the total number of epochs, simply run code steps 1 - 4 again as soon as possible.  The Python code in this Colab notebook will automatically detect the last completed epoch from the files placed in the `LSTM` folder in your Google Drive and will begin training the next epoch in sequence.\n",
        "\n",
        "  Repeat this as many times as necessary until all 10 epochs have been successfully completed.\n",
        "\n",
        "<br>\n",
        "\n",
        "### **Part II:  Analyze**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Complete these tasks:**\n",
        "\n",
        "1.  Once the model training is complete, record the descriptive data:\n",
        "\n",
        "    *  Total number of tokens\n",
        "    *  Total number of unique words (vocabulary)\n",
        "    *  Type-to-token ratio (TTR)\n",
        "\n",
        "\n",
        "2.  Answer the following questions:\n",
        "\n",
        "  <ol type=\"a\">\n",
        "    <li>On average and compared with other texts, is the TTR value for this text high, low, or somewhere in between?  (You may want to look back at your results from the Heaps' Law assignment to get a comparison.)</li>\n",
        "\n",
        "    <li>What does the TTR say about these first four chapters of 1 Nephi?</li>\n",
        "  </ol>\n",
        "\n",
        "\n",
        "3.  Run code step 5 to plot the loss curve.  Copy the loss curve plot to your report, then answer the following questions:\n",
        "\n",
        "  <ol type=\"a\">\n",
        "    <li>Are there any unusual, unexpected, or interesting features of the loss curve?  If so, what are they?  Do they correspond to any events during the model training?</li>\n",
        "\n",
        "    <li>Does the loss for every epoch decrease or are there epochs where the loss increases compared to the prior epoch?</li>\n",
        "\n",
        "    <li>Does the loss for the last epoch decrease or increase compared to the prior epoch?  Is this result expected?</li>\n",
        "  </ol>\n",
        "\n",
        "4. Describe the architecture of the LSTM neural network based upon the hyperparameter values specified in code step 3.\n",
        "\n",
        "5.  Answer these questions:  \n",
        "\n",
        "  <ol type=\"a\">\n",
        "    <li>What is the purpose of the learning rate (learn_rate) parameter and what does the specified value mean?</li>\n",
        "\n",
        "    <li>Describe at least 2 ways to improve the performance (speed and accuracy) of the LSTM training and the resulting model.</li>\n",
        "  </ol>  \n",
        "\n",
        "<br>\n",
        "\n",
        "### **Part III:  Use the Model**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Complete these tasks:**\n",
        "\n",
        "1.  In code step 6, enter each of the seed prompts listed below into the `seed_text` text box, set the value of `num_output_tokens` to 5, run the code cell, and record the resulting generated text in your report.\n",
        "\n",
        "<br>\n",
        "\n",
        "<div align=\"center\">\n",
        "\n",
        "  | Number | Seed Prompt | Reference |\n",
        "  |----------|----------|----------:|\n",
        "  | 1 | And it came to pass | 1:6 |\n",
        "  | 2 | he built an altar of stones | 2:7 |\n",
        "  | 3 | Behold I have dreamed a dream | 3:2 |\n",
        "  | 4 | Laman went in unto the house of Laban | 3:11 |\n",
        "  | 5 | the words which have been spoken by the mouth of all the holy prophets | 3:20 |\n",
        "  | 6 | let us be faithful in keeping the commandments of the Lord | 4:1 |\n",
        "  | 7 | Inasmuch as thy seed shall keep my commandments | 4:14 |\n",
        "\n",
        "</div>\n",
        "\n",
        "<br>\n",
        "\n",
        "2.  Find three additional seed prompts from the text that you find interesting and record the resulting generated text in your report.  \n",
        "\n",
        "  (Safety tip:  Ensure that your seed prompts do not contain punctuation or numbers.)\n",
        "\n",
        "\n",
        "3.  Answer the following questions for all 10 results:\n",
        "\n",
        "  <ol type=\"a\">\n",
        "    <li>Are there any quality issues with the generated texts?  If so, describe the issues.</li>\n",
        "\n",
        "    <li>Which generated text is the best?  Justify your answer.  (For example, it is more grammatical, it seems to flow better than the others, etc.)</li>\n",
        "\n",
        "    <li>Are there any general shared characteristics among the generated texts?  Explain.</li>\n",
        "  </ol>\n",
        "\n",
        "4.  Repeat these tasks for the 10 seed prompts but changing the value of `num_output_tokens` to 3, then answer these questions:\n",
        "\n",
        "  <ol type=\"a\">\n",
        "    <li>Are the first 3 tokens of the generated texts (not counting the seed prompt portions) the same as the generated tokens when setting the number of output tokens to 5?</li>\n",
        "\n",
        "    <li>Are there any general shared characteristics among the generated texts?  Explain.</li>\n",
        "\n",
        "    <li>Are the outputs deterministic?  Explain.</li>\n",
        "  </ol>\n",
        "\n",
        "<br>\n",
        "\n",
        "### **(Optional) Part IV:  Play Time!**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Complete these *optional* tasks:**\n",
        "\n",
        "\n",
        "*This part is optional.  You are not required to complete this portion of the assignment, though it is encouraged.*\n",
        "\n",
        "\n",
        "You have now created several model versions, one version per epoch.  Each model version has different performance characteristics, with later models being generally more accurate than earlier models.\n",
        "\n",
        "  1.  Modify the code in code step 6 to load any epoch-specific model (e.g., `lstm_model_2.pth`) and send it to the `predict_next_words()` function as the `model` parameter.  (Hint:  You will need to use the LSTM framework as used in code step 3 along with the `model.load_state_dict()` function as used in code step 4.)\n",
        "  2.  Use that model to generate text using the seed prompts in Part III.\n",
        "  3.  Compare the generated text from at least 2 different models.  \n",
        "  \n",
        "    * Describe the unique and interesting features of each.\n",
        "    * Are there general patterns evident for a particular model?\n",
        "    * Which model produces the best results?  Is that surprising?\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "lHiLohpcsWkD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVuFPjHeZOmt"
      },
      "source": [
        "## **Code Step 1:  Load the Necessary Code Libraries**\n",
        "\n",
        "---\n",
        "\n",
        "***What this step does:***\n",
        "\n",
        "*  Loads the necessary Python libraries.\n",
        "\n",
        "*  Mounts your Google Drive for Colab access.\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Complete these tasks for this step:**\n",
        "1.   Run the code cell in this step.  This step needs to be run only once.\n",
        "\n",
        "2.   You will be prompted to give access to your Google Drive so that the training can read and write files that will persist beyond the life of the runtime.  Provide access to your Google Drive for reading and writing files for training and reporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G_oGk2EPBqkQ"
      },
      "outputs": [],
      "source": [
        "# @title Import necessary libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import defaultdict\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import warnings\n",
        "from google.colab import drive\n",
        "\n",
        "# Ignore Python warnings (they get in the way)\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiZTP_7yZtaa"
      },
      "source": [
        "## **Code Step 2:  Define Classes and Functions**\n",
        "\n",
        "---\n",
        "\n",
        "***What this step does:***\n",
        "\n",
        "*  Defines the following classes:\n",
        "  * `TextDataset`:  Used for defining the training dataset.\n",
        "  * `LSTMModel`:  Used for defining the LSTM model.\n",
        "\n",
        "*  Defines the following functions:\n",
        "  * `clean_text()`:  Cleans the text and prepares it for processing.\n",
        "  * `get_latest_file()`:  Finds the latest file by using regular expressions to find numbers in the filenames, identify the highest number, and select the corresponding filename.\n",
        "  * `load_losses()`:  Loads the latest losses file.\n",
        "  * `train_model()`:  Trains the LSTM for use as a language model.\n",
        "  * `predict_next_words()`:  Once the LSTM model has been trained, uses the model to predict the next sequence of $n$ words (tokens).\n",
        "\n",
        "<br>\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Complete this task for this step:**\n",
        "1.   Run the code cell in this step.  This step needs to be run only once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0x855VFZi3_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Define Classes and Functions\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, xs, labels):\n",
        "        self.xs = xs\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.xs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.xs[idx], self.labels[idx]\n",
        "\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, total_words, embed_dim, hidden_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(total_words, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, total_words)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.fc(x[:, -1, :])\n",
        "\n",
        "        return x\n",
        "\n",
        "#===========================================================================\n",
        "def clean_text(text):\n",
        "    curlies = ['“', '”', '’']\n",
        "    end_of_line =[\"\\n\", \"\\r\"]\n",
        "\n",
        "    # Create a list of punctuation and digits characters\n",
        "    unwantedCharacters = list(string.punctuation)\n",
        "    unwantedCharacters.extend(list(string.digits))\n",
        "    unwantedCharacters.extend(curlies)\n",
        "\n",
        "    # Remove unwanted characters from the text\n",
        "    for character in unwantedCharacters:\n",
        "        text = text.replace(character, '').lower()\n",
        "\n",
        "    # Remove end of line characters\n",
        "    for character in end_of_line:\n",
        "        text = text.replace(character, ' ')\n",
        "\n",
        "    return text\n",
        "\n",
        "#===========================================================================\n",
        "# Get latest\n",
        "def get_latest_file(filenames):\n",
        "    # Extract the numbers from the filenames\n",
        "    numbers = [int(re.search(r'\\d+', filename).group()) for filename in filenames]\n",
        "\n",
        "    # Find the index of the maximum number\n",
        "    max_index = numbers.index(max(numbers))\n",
        "\n",
        "    # Get the max number\n",
        "    max_num = max(numbers)\n",
        "\n",
        "    # Select the filename with the highest number\n",
        "    selected_filename = filenames[max_index]\n",
        "\n",
        "    return selected_filename, max_num\n",
        "\n",
        "#===========================================================================\n",
        "def load_losses(losses_file, max_epoch, dir_path):\n",
        "    print(f\"Loading losses file: losses_{max_epoch}.txt\")\n",
        "\n",
        "    with open(dir_path + f'losses_{max_epoch}.txt', 'r') as file:\n",
        "        losses = [float(line.strip()) for line in file]\n",
        "\n",
        "    print(f\"Loaded losses file: losses_{max_epoch}.txt\")\n",
        "\n",
        "    return losses\n",
        "\n",
        "#===========================================================================\n",
        "def train_model(model, dataloader, criterion, optimizer, start, epochs, losses, dir_path):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    from time import time\n",
        "\n",
        "    if start != epochs:\n",
        "        print()\n",
        "        print(\"Training...\")\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "        total_batches = len(dataloader)\n",
        "\n",
        "        for epoch in range(start, epochs):\n",
        "            epoch_start_time = time()\n",
        "            total_loss = 0\n",
        "\n",
        "            print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "            print(\"-\" * 20)\n",
        "\n",
        "            for batch_idx, (xs_batch, labels_batch) in enumerate(dataloader):\n",
        "                # Move data to GPU if available\n",
        "                xs_batch = xs_batch.long().to(device)\n",
        "                labels_batch = labels_batch.long().to(device)\n",
        "\n",
        "                outputs = model(xs_batch)\n",
        "                loss = criterion(outputs, labels_batch)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Print progress every 100 batches\n",
        "                if (batch_idx + 1) % 100 == 0:\n",
        "                    progress = (batch_idx + 1) / total_batches * 100\n",
        "                    print(f\"Batch {batch_idx + 1}/{total_batches} ({progress:.1f}%) - Current loss: {loss.item():.4f}\")\n",
        "\n",
        "            # Calculate the average loss for the epoch\n",
        "            average_loss = total_loss / len(dataloader)\n",
        "            losses.append(average_loss)\n",
        "\n",
        "            epoch_time = time() - epoch_start_time\n",
        "\n",
        "            print(f'\\nEpoch {epoch + 1} Summary:')\n",
        "            print(f'Average Loss: {average_loss:.4f}')\n",
        "            print(f'Time taken: {epoch_time:.1f} seconds')\n",
        "\n",
        "            # Move model to CPU for saving\n",
        "            model.cpu()\n",
        "            torch.save(model.state_dict(), dir_path + f'lstm_model_{epoch + 1}.pth')\n",
        "            # Move model back to GPU\n",
        "            model.to(device)\n",
        "\n",
        "            # Save the losses list\n",
        "            with open(dir_path + f'losses_{epoch + 1}.txt', 'w') as file:\n",
        "                for number in losses:\n",
        "                    file.write(f\"{number}\\n\")\n",
        "\n",
        "    print()\n",
        "    print(f\"Training complete. The model has been trained for {epochs} epochs.\")\n",
        "\n",
        "#===========================================================================\n",
        "# Predict the next words in the sequence\n",
        "def predict_next_words(model, tokenizer, text, n):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "\n",
        "    for _ in range(n):\n",
        "        token_list = [tokenizer[word] for word in word_tokenize(text)]\n",
        "        token_list = np.pad(token_list, (max_sequence_len-len(token_list), 0), 'constant')\n",
        "        token_list = torch.LongTensor(token_list).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predicted = model(token_list)\n",
        "\n",
        "        predicted_word = [word for word, index in tokenizer.items() if index == torch.argmax(predicted).item()][0]\n",
        "        text += \" \" + predicted_word\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKlir_Jaa0ht"
      },
      "source": [
        "## **Code Step 3:  DASHBOARD: Load and Clean the Text and Set the Hyperparameters**\n",
        "\n",
        "---\n",
        "\n",
        "***What this step does:***\n",
        "\n",
        "*  Loads and cleans the text for processing.\n",
        "\n",
        "*  Defines model hyperparameters and function values.\n",
        "\n",
        "*  Calculates the Type-to-Token Ratio (TTR), which is the total number of unique tokens divided by the number of total tokens in a given corpus.\n",
        "\n",
        "*  Looks for losses and model files that have been previously generated.\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Complete this task for this step:**\n",
        "1.   Run the code cell in this step.  This step needs to be run only once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "t_cugJtLC47D"
      },
      "outputs": [],
      "source": [
        "# @title Load and Clean the Text and Set the Hyperparameters\n",
        "\n",
        "files_list = []\n",
        "losses_list = []\n",
        "models_list = []\n",
        "\n",
        "# List all files in the directory\n",
        "directory_path = '/content/drive/My Drive/LING 581/LSTM/'\n",
        "filenames = os.listdir(directory_path)\n",
        "\n",
        "# Add all filenames to the files list\n",
        "for filename in filenames:\n",
        "    root, extension = os.path.splitext(filename)\n",
        "\n",
        "    if extension == '.txt':\n",
        "        if filename.startswith('losses_'):\n",
        "            losses_list.append(filename)\n",
        "        else:\n",
        "            files_list.append(filename)\n",
        "\n",
        "    if extension == '.pth':\n",
        "        models_list.append(filename)\n",
        "\n",
        "print(f\"Number of text files: {len(files_list)}\")\n",
        "\n",
        "for file in files_list:\n",
        "    # Read text from a file\n",
        "    print(f\"Using text: {file}\")\n",
        "    print(\"Loading the text...\")\n",
        "\n",
        "    with open(directory_path + file, 'r', encoding=\"utf-8\") as file:\n",
        "        text = file.read()\n",
        "\n",
        "# Tokenize the text\n",
        "text = clean_text(text)\n",
        "tokens = word_tokenize(text)\n",
        "tokenizer = defaultdict(lambda: len(tokenizer))\n",
        "\n",
        "# Create word index\n",
        "sequence = [tokenizer[word] for word in tokens]\n",
        "total_words = len(tokenizer)\n",
        "\n",
        "# Calculate type-to-token ratio (TTR)\n",
        "ttr = total_words / len(tokens)\n",
        "\n",
        "# Prepare the input sequences\n",
        "input_sequences = []\n",
        "\n",
        "for i in range(1, len(sequence)):\n",
        "    n_gram_sequence = sequence[:i+1]\n",
        "    input_sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array([np.pad(x, (max_sequence_len-len(x), 0), 'constant') for x in input_sequences])\n",
        "\n",
        "# Split data into features and labels\n",
        "xs, labels = input_sequences[:,:-1], input_sequences[:,-1]\n",
        "\n",
        "dataset = TextDataset(xs, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Hyperparameters\n",
        "epochs = 14 # @param {\"type\":\"integer\"}\n",
        "embed_dim = 10 # @param {\"type\":\"integer\"}\n",
        "hidden_dim = 100 # @param {\"type\":\"integer\"}\n",
        "learn_rate = 0.01 # @param {\"type\":\"number\"}\n",
        "\n",
        "# LSTM framework\n",
        "model = LSTMModel(total_words, embed_dim, hidden_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate)\n",
        "\n",
        "print(f\"Total number of tokens: {len(tokens)}\")\n",
        "print(f\"Total number of unique words (vocabulary): {total_words}\")\n",
        "print(f\"Type-to-token ratio (TTR): {ttr}\")\n",
        "\n",
        "print()\n",
        "\n",
        "if len(losses_list) > 0:\n",
        "    print(\"Losses file found.\")\n",
        "\n",
        "if len(models_list) > 0:\n",
        "    print(\"Model file found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P0kewnkbV_M"
      },
      "source": [
        "## **Code Step 4:  Train the LSTM Model**\n",
        "\n",
        "---\n",
        "\n",
        "***What this step does:***\n",
        "\n",
        "Trains the LSTM model with the specified hyperparameters given the input text.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Complete this task for this step:**\n",
        "1.   Run the code cell in this step.  This step needs to be run only once.  Note that this may take several minutes for each epoch using the base CPU runtime; you might consider using a GPU or TPU runtime to speed up the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BzvbZRZvNr1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Train the Model\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Move model to GPU if available\n",
        "model = model.to(device)\n",
        "\n",
        "losses = []\n",
        "start = 0\n",
        "\n",
        "if len(models_list) > 0:\n",
        "    model_file, max_epoch = get_latest_file(models_list)\n",
        "\n",
        "    if max_epoch < epochs:\n",
        "        cont = input(f\"A model file was found with fewer completed epochs ({max_epoch}) than the specified epochs parameter ({epochs}).  Continue training? (y/n)  \").lower()\n",
        "        print()\n",
        "\n",
        "        if cont == 'y':\n",
        "            print(f\"Loading model: {model_file}\")\n",
        "            # Load model to GPU if available\n",
        "            model.load_state_dict(torch.load(directory_path + model_file, map_location=device))\n",
        "            print(f\"Loaded model: {model_file}\")\n",
        "\n",
        "            losses_file, max_epoch = get_latest_file(losses_list)\n",
        "            losses = load_losses(losses_file, max_epoch, directory_path)\n",
        "\n",
        "            start = max_epoch\n",
        "\n",
        "            train_model(model, dataloader, criterion, optimizer, start, epochs, losses, directory_path)\n",
        "        else:\n",
        "            print(f\"Training stopped.  The current model has completed {max_epoch} out of {epochs} epochs.\")\n",
        "    else:\n",
        "        print()\n",
        "        print(f\"Training complete.  The model has been trained for {epochs} epochs.\")\n",
        "else:\n",
        "    train_model(model, dataloader, criterion, optimizer, start, epochs, losses, directory_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code Step 5:  Plot the Loss Curve**\n",
        "\n",
        "---\n",
        "\n",
        "***What this step does:***\n",
        "\n",
        "Plots the loss curve for the LSTM training.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Complete this task for this step:**\n",
        "1.   Run the code cell in this step."
      ],
      "metadata": {
        "id": "zjW8aQ1NL0y5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Plot the Loss Curve\n",
        "\n",
        "# Re-scan the directory for loss files\n",
        "losses_list = []  # Reinitialize the list\n",
        "\n",
        "# List all files in the directory again\n",
        "filenames = os.listdir(directory_path)\n",
        "\n",
        "# Add all loss filenames to the losses_list\n",
        "for filename in filenames:\n",
        "    root, extension = os.path.splitext(filename)\n",
        "    if extension == '.txt' and filename.startswith('losses_'):\n",
        "        losses_list.append(filename)\n",
        "\n",
        "if len(losses_list) > 0:\n",
        "    losses_file, max_epoch = get_latest_file(losses_list)\n",
        "    losses = load_losses(losses_file, max_epoch, directory_path)\n",
        "    print()\n",
        "\n",
        "    x_range = range(1, len(losses) + 1)\n",
        "    plt.plot(x_range, losses)\n",
        "    plt.xlim(1, epochs)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('LSTM Training Loss Curve')\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"No losses file found.\")\n",
        "    print(\"Unable to plot, insufficient data.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "SPtzDQcqUitk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Code Step 6:  Predict Next Words**\n",
        "\n",
        "---\n",
        "\n",
        "***What this step does:***\n",
        "\n",
        "Generates the number of output text tokens given the input seed text.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Complete these tasks for this step:**\n",
        "1.   Specify the number of output tokens.\n",
        "2.   Provide the seed text.\n",
        "3.   Run the code cell in this step."
      ],
      "metadata": {
        "id": "-sYdS6RKLIvN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bkKDaW3DvDZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Predict Next Words\n",
        "# Example usage\n",
        "\n",
        "num_output_tokens = 5 # @param {\"type\":\"integer\"}\n",
        "\n",
        "seed_text = \"\" # @param {\"type\":\"string\"}\n",
        "predicted_text = predict_next_words(model, tokenizer, seed_text.lower(), num_output_tokens)\n",
        "print(predicted_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLHUjMTuQg5F"
      },
      "source": [
        "#  \n",
        "\n",
        "|  |  |\n",
        "|----------|----------|\n",
        "| Colab notebook created by:|\n",
        "| <img src=\"https://brightspotcdn.byu.edu/8e/28/7bcd62fe4b2b9517b74f783decfe/1-monogram-378w.svg\" alt=\"BYU Logo\" width=\"150\">|Professor Duane K. Dougal<br>Computer Science Department & Department of Linguistics<br>Brigham Young University|\n",
        "|||\n",
        "||CUDA improvements by: Brock Rawson|"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}